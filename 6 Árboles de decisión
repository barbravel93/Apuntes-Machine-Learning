#Vamos a construir, en este ejemplo práctico, un árbol de decisión que nos ayudará a identificar cuál de los 5 medicamentos posibles para tratar una enfermedad será el que\
  \mejor eficiencia tenga para cada paciente individual teniendo en cuenta un conjunto de características.
  
#El primer paso será importar las bibliotecas necesarias
import numpy as np 
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

#A continuación, descargamos el conjunto de datos que vamos a utilizar
!wget -O drug200.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv

#Procedemos a dataframear los datos para facilitar su lectura y a echarles un primer vistazo
my_data = pd.read_csv("drug200.csv", delimiter=",")
my_data[0:5]

#Veamos el tamaño de nuestro conjunto de datos
my_data.size
  #[OUT]: 1200
  
#Pasamos al preprocesamiento de datos. En este paso, seleccionaremos nuestro target (la variable a predecir) y crearemos la variable X, donde almacenaremos nuestras variables independientes (matriz de características)
X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values
X[0:5]
  #[OUT]: array([[23, 'F', 'HIGH', 'HIGH', 25.355],
       [47, 'M', 'LOW', 'HIGH', 13.093],
       [47, 'M', 'LOW', 'HIGH', 10.114],
       [28, 'F', 'NORMAL', 'HIGH', 7.798],
       [61, 'F', 'LOW', 'HIGH', 18.043]], dtype=object)
       
#Dado qu algunos de nuestros datos son categóricos, y que scikitlearn decision trees no admite este tipo de variables, recurriremos al método pandas.get_dummies() para convertir estas variables en variables indicadoras
  #!Variable indicadora: Ej. Sex = M/F ----> Convertimos M en 0 y F en 1
from sklearn import preprocessing
le_sex = preprocessing.LabelEncoder()
le_sex.fit(['F','M'])
X[:,1] = le_sex.transform(X[:,1]) 

le_BP = preprocessing.LabelEncoder()
le_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])
X[:,2] = le_BP.transform(X[:,2])

le_Chol = preprocessing.LabelEncoder()
le_Chol.fit([ 'NORMAL', 'HIGH'])
X[:,3] = le_Chol.transform(X[:,3]) 

X[0:5]
  #[OUT]:array([[23, 0, 0, 0, 25.355],
       [47, 1, 1, 0, 13.093],
       [47, 1, 1, 0, 10.114],
       [28, 0, 2, 0, 7.798],
       [61, 0, 1, 0, 18.043]], dtype=object)
       

#Ya podemos categorizar nuestra variable objetivo
y = my_data["Drug"]
y[0:5]
  #[OUT]: 0    drugY
          1    drugC
          2    drugC
          3    drugX
          4    drugY
          Name: Drug, dtype: object
          
#Configuremos nuestro árbol de decisión
#Lo primero será descargar la biblioteca de trains/Split set
from sklearn.model_selection import train_test_split

#Ahora train_test_split devolverá 4 parámetros diferentes. Los nombraremos: X_trainset, X_testset, y_trainset, y_testset\
  \El train_test_split necesitará los parámetros:X, y, test_size = 0.3 y random_state = 3.
  \X e y son las matrices requeridas antes de la división, test_size representa la proporción del conjunto de datos de prueba y random_state asegura que obtengamos las mismas divisiones.
X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)

#Veamos que forma tiene nuestro set
print('Shape of X training set {}'.format(X_trainset.shape),'&',' Size of Y training set {}'.format(y_trainset.shape))
  #[OUT]: Shape of X training set (140, 5) &  Size of Y training set (140,)
print('Shape of X testing set{}'.format(X_testset.shape), '&', 'Size of Y testing set {}'.format(y_testset.shape))
  #[OUT]: Shape of X testing set(60, 5) & Size of Y testing set (60,)

# Para el modelado, vamos a crear una instancia de DecisionTreeClasifier a la que llamaremos drugTree.
  #En el clasificador vamos a indicar el criterio de entropía para observar la ganancia de información en cada nodo.
drugTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
drugTree
  #[OUT]:DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
           
# A continuación, ajustaremos los datos con la matriz de funciones de entrenamiento X_trainset y el vector de respuesta de entrenamiento y_trainset
drugTree.fit(X_trainset,y_trainset)

#Hagamos algunas predicciones con el set de prueba y almacenémoslas en la variable predTree
predTree = drugTree.predict(X_testset)

#Podemos ver, ahora, si las predicciones de nuestro árbol coinciden con los valores actuales de nuestros datos
print (predTree [0:5])
print (y_testset [0:5])
  #[OUT]: ['drugY' 'drugX' 'drugX' 'drugX' 'drugX']
          40     drugY
          51     drugX
          139    drugX
          197    drugX
          170    drugX
          Name: Drug, dtype: object
          
#A continuación vamos a importar algunas métricas y bibliotecas para evaluar nuestro modelo
from sklearn import metrics
import matplotlib.pyplot as plt

#Evaluémoslo
print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_testset, predTree))
  #[OUT]: DecisionTrees's Accuracy:  0.9833333333333333
  #!!!^^^^ README
  
#Now, lets visualize the tree. For this, first we need to install some libraries
!conda install -c conda-forge pydotplus -y
!conda install -c conda-forge python-graphviz -y

#Luego, importaremos algunos módulos
from  io import StringIO
import pydotplus
import matplotlib.image as mpimg
from sklearn import tree
%matplotlib inline 

#A continuación, con las siguientes lineas de código podremos trazar nuestro árbol
dot_data = StringIO()
filename = "drugtree.png"
featureNames = my_data.columns[0:5]
out=tree.export_graphviz(drugTree,feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_trainset), filled=True,  special_characters=True,rotate=False)  
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png(filename)
img = mpimg.imread(filename)
plt.figure(figsize=(100, 200))
plt.imshow(img,interpolation='nearest')

